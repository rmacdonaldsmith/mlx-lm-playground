"""
ArtifactEmitter Node (Deterministic)

Emits final JSON test plan and optional test skeletons.
This node packages all generated artifacts into the final output format
with stable, reproducible naming and structure.
"""

from __future__ import annotations
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from datetime import datetime, timezone
import logging

from ..models import (
    RequirementsInput,
    AcceptanceCriteria,
    Scenario,
    TestCase,
    CoverageMap,
    OpenQuestion,
    TestPlan,
    Constraints
)

logger = logging.getLogger(__name__)


class TestSkeletonGenerator:
    """Generates test skeleton code for various frameworks."""
    
    @staticmethod
    def generate_skeleton(
        test_cases: List[TestCase],
        framework: str,
        language: str = "python"
    ) -> Dict[str, str]:
        """
        Generate test skeleton code.
        
        Args:
            test_cases: Test cases to generate skeletons for
            framework: Target test framework
            language: Programming language
            
        Returns:
            Dict mapping filename to file content
        """
        
        if framework.lower() == "playwright":
            return TestSkeletonGenerator._generate_playwright_skeleton(test_cases)
        elif framework.lower() == "pytest":
            return TestSkeletonGenerator._generate_pytest_skeleton(test_cases)
        elif framework.lower() == "selenium":
            return TestSkeletonGenerator._generate_selenium_skeleton(test_cases)
        elif framework.lower() == "cypress":
            return TestSkeletonGenerator._generate_cypress_skeleton(test_cases)
        else:
            return TestSkeletonGenerator._generate_generic_skeleton(test_cases, framework)
    
    @staticmethod
    def _generate_playwright_skeleton(test_cases: List[TestCase]) -> Dict[str, str]:
        """Generate Playwright test skeletons."""
        
        imports = '''"""
Generated test skeletons for Playwright.
Auto-generated by QA Test Scenario Generator.
"""

import pytest
from playwright.sync_api import Page, expect


'''
        
        test_functions = []
        
        for tc in test_cases:
            # Create function name from test ID
            func_name = f"test_{tc.id.lower().replace('-', '_')}"
            
            # Build test function
            func_code = f'''def {func_name}(page: Page):
    """
    Test Case: {tc.id}
    Scenario: {tc.scenario_id}
    Type: {tc.case_type}
    Priority: {tc.priority}
    Negative: {tc.negative}
    """
    
    # Test Steps:
'''
            
            # Add steps as comments and basic implementation
            for i, step in enumerate(tc.steps, 1):
                func_code += f"    # Step {i}: {step}\n"
                
                # Try to convert step to basic Playwright code
                step_lower = step.lower()
                if "click" in step_lower and "button" in step_lower:
                    func_code += f'    # page.click("button")\n'
                elif "fill" in step_lower or "enter" in step_lower or "type" in step_lower:
                    func_code += f'    # page.fill("input", "value")\n'
                elif "navigate" in step_lower or "visit" in step_lower:
                    func_code += f'    # page.goto("url")\n'
                else:
                    func_code += f'    # TODO: Implement this step\n'
            
            func_code += "\n    # Expected Results:\n"
            for i, expected in enumerate(tc.expected, 1):
                func_code += f"    # {i}. {expected}\n"
                # Add basic assertions
                if "error" in expected.lower() or "message" in expected.lower():
                    func_code += f'    # expect(page.locator(".error")).to_be_visible()\n'
                elif "redirect" in expected.lower():
                    func_code += f'    # expect(page).to_have_url("expected-url")\n'
                else:
                    func_code += f'    # expect(page.locator("selector")).to_be_visible()\n'
            
            func_code += "\n    # TODO: Implement complete test logic\n    pass\n\n"
            test_functions.append(func_code)
        
        content = imports + "\n".join(test_functions)
        return {"test_generated.py": content}
    
    @staticmethod
    def _generate_pytest_skeleton(test_cases: List[TestCase]) -> Dict[str, str]:
        """Generate pytest test skeletons."""
        
        imports = '''"""
Generated test skeletons for pytest.
Auto-generated by QA Test Scenario Generator.
"""

import pytest


'''
        
        test_functions = []
        
        for tc in test_cases:
            func_name = f"test_{tc.id.lower().replace('-', '_')}"
            
            func_code = f'''def {func_name}():
    """
    Test Case: {tc.id}
    Scenario: {tc.scenario_id}
    Type: {tc.case_type}
    Priority: {tc.priority}
    Negative: {tc.negative}
    
    Steps:
'''
            
            for i, step in enumerate(tc.steps, 1):
                func_code += f"    {i}. {step}\n"
            
            func_code += "    \n    Expected:\n"
            for i, expected in enumerate(tc.expected, 1):
                func_code += f"    {i}. {expected}\n"
            
            func_code += '    """\n    \n    # TODO: Implement test logic\n'
            
            # Add basic assertions based on negative flag
            if tc.negative:
                func_code += "    # Test should verify error condition\n"
                func_code += "    # with pytest.raises(ExpectedException):\n"
                func_code += "    #     perform_action()\n"
            else:
                func_code += "    # Test should verify successful operation\n"
                func_code += "    # result = perform_action()\n"
                func_code += "    # assert result == expected_value\n"
            
            func_code += "    \n    assert False, 'Test not implemented'\n\n"
            test_functions.append(func_code)
        
        content = imports + "\n".join(test_functions)
        return {"test_generated.py": content}
    
    @staticmethod
    def _generate_selenium_skeleton(test_cases: List[TestCase]) -> Dict[str, str]:
        """Generate Selenium test skeletons."""
        
        imports = '''"""
Generated test skeletons for Selenium WebDriver.
Auto-generated by QA Test Scenario Generator.
"""

import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


class TestGenerated:
    
    @pytest.fixture(autouse=True)
    def setup_and_teardown(self):
        self.driver = webdriver.Chrome()  # Configure as needed
        self.wait = WebDriverWait(self.driver, 10)
        yield
        self.driver.quit()

'''
        
        test_methods = []
        
        for tc in test_cases:
            method_name = f"test_{tc.id.lower().replace('-', '_')}"
            
            method_code = f'''    def {method_name}(self):
        """
        Test Case: {tc.id}
        Scenario: {tc.scenario_id}
        Type: {tc.case_type}
        Priority: {tc.priority}
        Negative: {tc.negative}
        """
        
        # Test Steps:
'''
            
            for i, step in enumerate(tc.steps, 1):
                method_code += f"        # Step {i}: {step}\n"
                
                # Basic Selenium code generation
                step_lower = step.lower()
                if "click" in step_lower:
                    method_code += f'        # self.driver.find_element(By.ID, "element_id").click()\n'
                elif "enter" in step_lower or "type" in step_lower:
                    method_code += f'        # self.driver.find_element(By.ID, "input_id").send_keys("value")\n'
                elif "navigate" in step_lower:
                    method_code += f'        # self.driver.get("url")\n'
                else:
                    method_code += f'        # TODO: Implement this step\n'
            
            method_code += "\n        # Expected Results:\n"
            for i, expected in enumerate(tc.expected, 1):
                method_code += f"        # {i}. {expected}\n"
                method_code += f'        # assert self.wait.until(EC.presence_of_element_located((By.ID, "element")))\n'
            
            method_code += "\n        # TODO: Implement complete test logic\n        assert False, 'Test not implemented'\n\n"
            test_methods.append(method_code)
        
        content = imports + "\n".join(test_methods)
        return {"test_generated.py": content}
    
    @staticmethod
    def _generate_cypress_skeleton(test_cases: List[TestCase]) -> Dict[str, str]:
        """Generate Cypress test skeletons."""
        
        content = '''// Generated test skeletons for Cypress
// Auto-generated by QA Test Scenario Generator

'''
        
        for tc in test_cases:
            test_name = tc.id
            
            content += f'''describe('{tc.scenario_id}', () => {{
  it('{test_name}: {tc.title if hasattr(tc, "title") else tc.id}', () => {{
    /*
     * Test Case: {tc.id}
     * Type: {tc.case_type}
     * Priority: {tc.priority}
     * Negative: {tc.negative}
     */
    
    // Test Steps:
'''
            
            for i, step in enumerate(tc.steps, 1):
                content += f"    // {i}. {step}\n"
                
                # Basic Cypress code generation
                step_lower = step.lower()
                if "visit" in step_lower or "navigate" in step_lower:
                    content += f"    // cy.visit('url')\n"
                elif "click" in step_lower:
                    content += f"    // cy.get('selector').click()\n"
                elif "type" in step_lower or "enter" in step_lower:
                    content += f"    // cy.get('input').type('value')\n"
                else:
                    content += f"    // TODO: Implement this step\n"
            
            content += "\n    // Expected Results:\n"
            for i, expected in enumerate(tc.expected, 1):
                content += f"    // {i}. {expected}\n"
                content += f"    // cy.get('selector').should('be.visible')\n"
            
            content += "\n    // TODO: Implement complete test logic\n"
            content += "    cy.log('Test not implemented')\n"
            content += "  })\n})\n\n"
        
        return {"generated.cy.js": content}
    
    @staticmethod
    def _generate_generic_skeleton(test_cases: List[TestCase], framework: str) -> Dict[str, str]:
        """Generate generic test skeleton."""
        
        content = f'''# Generated test skeletons for {framework}
# Auto-generated by QA Test Scenario Generator

'''
        
        for tc in test_cases:
            content += f'''
# Test Case: {tc.id}
# Scenario: {tc.scenario_id}
# Type: {tc.case_type}
# Priority: {tc.priority}
# Negative: {tc.negative}

def test_{tc.id.lower().replace('-', '_')}():
    """
    Test Steps:
'''
            
            for i, step in enumerate(tc.steps, 1):
                content += f"    {i}. {step}\n"
            
            content += "    \n    Expected Results:\n"
            for i, expected in enumerate(tc.expected, 1):
                content += f"    {i}. {expected}\n"
            
            content += '    """\n    \n    # TODO: Implement test logic\n    pass\n\n'
        
        return {"test_skeleton.py": content}


class ArtifactEmitter:
    """
    Node 5: Emit final artifacts (JSON plan + optional test skeletons).
    
    This is the final node that packages all generated artifacts into
    the output format with stable, deterministic structure.
    """
    
    def __init__(self, output_dir: Optional[Path] = None):
        """
        Args:
            output_dir: Directory to write artifacts (default: current directory)
        """
        self.output_dir = output_dir or Path.cwd()
    
    def process(
        self,
        requirements_input: RequirementsInput,
        acceptance_criteria: List[AcceptanceCriteria],
        scenarios: List[Scenario],
        test_cases: List[TestCase],
        coverage_map: CoverageMap,
        open_questions: List[OpenQuestion],
        constraints: Optional[Constraints] = None
    ) -> Dict[str, Any]:
        """
        Emit final test plan and artifacts.
        
        Args:
            requirements_input: Original requirements input
            acceptance_criteria: Normalized ACs
            scenarios: Generated scenarios  
            test_cases: Generated test cases
            coverage_map: Coverage analysis
            open_questions: Open questions
            constraints: Generation constraints
            
        Returns:
            Dict containing artifact paths and metadata
        """
        
        # Create complete test plan
        test_plan = self._build_test_plan(
            requirements_input, acceptance_criteria, scenarios, 
            test_cases, coverage_map, open_questions
        )
        
        # Write JSON plan
        json_path = self._write_json_plan(test_plan, requirements_input.artifact_id)
        
        # Generate test skeletons if requested
        skeleton_paths = {}
        if constraints and constraints.test_framework:
            skeleton_paths = self._write_test_skeletons(
                test_cases, constraints.test_framework, requirements_input.artifact_id
            )
        
        # Generate summary report
        summary = self._generate_summary(test_plan, json_path, skeleton_paths)
        
        logger.info(f"Artifacts generated: JSON plan at {json_path}")
        if skeleton_paths:
            logger.info(f"Test skeletons: {list(skeleton_paths.keys())}")
        
        return {
            "json_plan_path": str(json_path),
            "skeleton_paths": {k: str(v) for k, v in skeleton_paths.items()},
            "summary": summary,
            "test_plan": test_plan
        }
    
    def _build_test_plan(
        self,
        requirements_input: RequirementsInput,
        acceptance_criteria: List[AcceptanceCriteria],
        scenarios: List[Scenario],
        test_cases: List[TestCase],
        coverage_map: CoverageMap,
        open_questions: List[OpenQuestion]
    ) -> TestPlan:
        """Build complete test plan object."""
        
        # Generate metadata
        metadata = {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "generator_version": "0.1.0",
            "total_scenarios": len(scenarios),
            "total_test_cases": len(test_cases),
            "total_questions": len(open_questions),
            "coverage_stats": {
                "acs_covered": len(coverage_map.ac_to_scenarios),
                "scenarios_per_ac": {
                    ac_id: len(scenario_ids) 
                    for ac_id, scenario_ids in coverage_map.ac_to_scenarios.items()
                },
                "positive_test_cases": len([tc for tc in test_cases if not tc.negative]),
                "negative_test_cases": len([tc for tc in test_cases if tc.negative]),
                "priority_distribution": self._calculate_priority_distribution(test_cases)
            }
        }
        
        return TestPlan(
            project=requirements_input.project,
            artifact_id=requirements_input.artifact_id,
            acceptance_criteria=acceptance_criteria,
            scenarios=scenarios,
            test_cases=test_cases,
            coverage_map=coverage_map,
            open_questions=open_questions,
            metadata=metadata
        )
    
    def _calculate_priority_distribution(self, test_cases: List[TestCase]) -> Dict[str, int]:
        """Calculate distribution of test case priorities."""
        distribution = {"P0": 0, "P1": 0, "P2": 0, "P3": 0}
        
        for tc in test_cases:
            if tc.priority in distribution:
                distribution[tc.priority] += 1
        
        return distribution
    
    def _write_json_plan(self, test_plan: TestPlan, artifact_id: str) -> Path:
        """Write test plan to JSON file."""
        
        # Create filename with artifact ID
        filename = f"qa_test_plan_{artifact_id}.json"
        json_path = self.output_dir / filename
        
        # Ensure output directory exists
        json_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Write JSON with pretty formatting
        with json_path.open('w', encoding='utf-8') as f:
            json.dump(
                test_plan.dict(), 
                f, 
                indent=2, 
                ensure_ascii=False,
                sort_keys=True  # For deterministic output
            )
        
        return json_path
    
    def _write_test_skeletons(
        self, 
        test_cases: List[TestCase], 
        framework: str,
        artifact_id: str
    ) -> Dict[str, Path]:
        """Write test skeleton files."""
        
        # Generate skeletons
        skeletons = TestSkeletonGenerator.generate_skeleton(test_cases, framework)
        
        # Write files
        skeleton_paths = {}
        skeleton_dir = self.output_dir / f"test_skeletons_{artifact_id}"
        skeleton_dir.mkdir(parents=True, exist_ok=True)
        
        for filename, content in skeletons.items():
            file_path = skeleton_dir / filename
            
            with file_path.open('w', encoding='utf-8') as f:
                f.write(content)
            
            skeleton_paths[filename] = file_path
        
        return skeleton_paths
    
    def _generate_summary(
        self, 
        test_plan: TestPlan, 
        json_path: Path,
        skeleton_paths: Dict[str, Path]
    ) -> Dict[str, Any]:
        """Generate summary of generated artifacts."""
        
        metadata = test_plan.metadata or {}
        coverage_stats = metadata.get("coverage_stats", {})
        
        return {
            "project": test_plan.project,
            "artifact_id": test_plan.artifact_id,
            "generated_at": metadata.get("generated_at"),
            "files_created": {
                "json_plan": str(json_path),
                "test_skeletons": list(skeleton_paths.keys())
            },
            "statistics": {
                "acceptance_criteria": len(test_plan.acceptance_criteria),
                "scenarios": len(test_plan.scenarios),
                "test_cases": len(test_plan.test_cases),
                "open_questions": len(test_plan.open_questions),
                "coverage_percentage": (
                    coverage_stats.get("acs_covered", 0) / 
                    len(test_plan.acceptance_criteria) * 100
                    if test_plan.acceptance_criteria else 0
                ),
                "priority_distribution": coverage_stats.get("priority_distribution", {}),
                "positive_vs_negative": {
                    "positive": coverage_stats.get("positive_test_cases", 0),
                    "negative": coverage_stats.get("negative_test_cases", 0)
                }
            }
        }


# Convenience functions

def emit_test_plan(
    requirements_input: RequirementsInput,
    acceptance_criteria: List[AcceptanceCriteria],
    scenarios: List[Scenario],
    test_cases: List[TestCase],
    coverage_map: CoverageMap,
    open_questions: List[OpenQuestion],
    constraints: Optional[Constraints] = None,
    output_dir: Optional[Path] = None
) -> Dict[str, Any]:
    """Convenience function to emit test plan."""
    emitter = ArtifactEmitter(output_dir)
    return emitter.process(
        requirements_input, acceptance_criteria, scenarios,
        test_cases, coverage_map, open_questions, constraints
    )